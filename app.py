# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XpN3qsuTQ9D4r4dkrKSIh7wbxSjcKsMq
"""

# -*- coding: utf-8 -*-
"""
app.py

CSV EDA Agent Streamlit App
- Permite upload de CSVs (ou ZIPs contendo CSVs)
- Executa EDA automático (estatísticas, histogramas, correlações, clusters, detecção de outliers)
- Integra com LangChain/OpenAI criando um CSV agent para perguntas em NL (quando disponível)
- Mantém memória local das conclusões em JSON
"""

import os
import io
import json
import zipfile
import tempfile
from typing import Optional, Tuple, List

import streamlit as st
import pandas as pd
import numpy as np
from dotenv import load_dotenv
import openai

try:
    import matplotlib.pyplot as plt
except ImportError:
    st.error("Matplotlib não instalado. Instalando...")
    import subprocess
    import sys
    subprocess.check_call([sys.executable, "-m", "pip", "install", "matplotlib"])
    import matplotlib.pyplot as plt

from sklearn.cluster import KMeans
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# LangChain & OpenAI imports (robusto para várias versões)
LANGCHAIN_CSV_AVAILABLE = False
try:
    from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent  # type: ignore
    from langchain.chat_models import ChatOpenAI  # type: ignore
    LANGCHAIN_CSV_AVAILABLE = True
except Exception:
    try:
        from langchain.agents import create_csv_agent  # type: ignore
        from langchain.chat_models import ChatOpenAI  # type: ignore
        LANGCHAIN_CSV_AVAILABLE = True
    except Exception:
        LANGCHAIN_CSV_AVAILABLE = False


# --- utils ---
def save_memory(dataset_id: str, memory_obj: dict, memory_dir: str = "memory"):
    os.makedirs(memory_dir, exist_ok=True)
    path = os.path.join(memory_dir, f"{dataset_id}_memory.json")
    with open(path, "w", encoding="utf-8") as f:
        json.dump(memory_obj, f, indent=2, ensure_ascii=False)


def load_memory(dataset_id: str, memory_dir: str = "memory") -> dict:
    path = os.path.join(memory_dir, f"{dataset_id}_memory.json")
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"conclusions": [], "qa": []}


def detect_outliers_isolation(df: pd.DataFrame, numeric_cols: List[str]) -> pd.Series:
    if len(numeric_cols) == 0:
        return pd.Series([False] * len(df), index=df.index)
    clf = IsolationForest(random_state=42, n_estimators=100)
    vals = df[numeric_cols].dropna()
    scaler = StandardScaler()
    try:
        vals_scaled = scaler.fit_transform(vals)
        preds = clf.fit_predict(vals_scaled)
        mask = pd.Series(False, index=df.index)
        mask.loc[vals.index] = preds == -1
        return mask
    except Exception:
        return pd.Series([False] * len(df), index=df.index)


def plot_histograms(df: pd.DataFrame, numeric_cols: List[str], max_cols: int = 6) -> List[str]:
    saved = []
    cols = numeric_cols[:max_cols]
    for c in cols:
        fig, ax = plt.subplots()
        try:
            df[c].dropna().hist(bins=30, ax=ax)
        except Exception:
            ax.text(0.5, 0.5, "Erro ao desenhar histograma", ha="center")
        ax.set_title(f"Histograma: {c}")
        fname = f"outputs/hist_{c}.png"
        os.makedirs(os.path.dirname(fname), exist_ok=True)
        fig.savefig(fname, bbox_inches="tight")
        plt.close(fig)
        saved.append(fname)
    return saved


def compute_basic_stats(df: pd.DataFrame) -> dict:
    stats = {}
    for c in df.columns:
        try:
            ser = df[c]
            is_numeric = pd.api.types.is_numeric_dtype(ser)
            stats[c] = {
                "dtype": str(ser.dtype),
                "count": int(ser.count()),
                "n_unique": int(ser.nunique()) if not is_numeric or ser.nunique() < 1e6 else None,
                "min": None if not is_numeric else float(ser.min()) if ser.count() > 0 else None,
                "max": None if not is_numeric else float(ser.max()) if ser.count() > 0 else None,
                "mean": None if not is_numeric else float(ser.mean()) if ser.count() > 0 else None,
                "median": None if not is_numeric else float(ser.median()) if ser.count() > 0 else None,
                "std": None if not is_numeric else float(ser.std()) if ser.count() > 0 else None,
            }
        except Exception as e:
            stats[c] = {"error": str(e)}
    return stats


def automatic_eda(df: pd.DataFrame, name: Optional[str] = None) -> dict:
    numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
    categorical_cols = df.select_dtypes(exclude=["number"]).columns.tolist()

    results = {
        "n_rows": int(df.shape[0]),
        "n_cols": int(df.shape[1]),
        "numeric_cols": numeric_cols,
        "categorical_cols": categorical_cols,
    }

    results['basic_stats'] = compute_basic_stats(df)
    results['histograms'] = plot_histograms(df, numeric_cols, max_cols=6)

    if len(numeric_cols) >= 2:
        corr = df[numeric_cols].corr()
        results['correlation_matrix'] = corr.to_dict()
        fig, ax = plt.subplots(figsize=(8, 6))
        cax = ax.matshow(corr.fillna(0))
        fig.colorbar(cax)
        ax.set_xticks(range(len(numeric_cols)))
        ax.set_yticks(range(len(numeric_cols)))
        ax.set_xticklabels(numeric_cols, rotation=90)
        ax.set_yticklabels(numeric_cols)
        os.makedirs('outputs', exist_ok=True)
        corr_fname = 'outputs/correlation_matrix.png'
        fig.savefig(corr_fname, bbox_inches='tight')
        plt.close(fig)
        results['correlation_image'] = corr_fname

    outlier_mask = detect_outliers_isolation(df, numeric_cols)
    results['n_outliers'] = int(outlier_mask.sum())
    results['outlier_indices_sample'] = df[outlier_mask].head(10).to_dict(orient='records')

    if len(numeric_cols) >= 2:
        try:
            X = df[numeric_cols].dropna()
            scaler = StandardScaler()
            Xs = scaler.fit_transform(X)
            km = KMeans(n_clusters=3, random_state=42, n_init='auto')
            labels = km.fit_predict(Xs)
            from sklearn.decomposition import PCA
            pca = PCA(n_components=2)
            proj = pca.fit_transform(Xs)
            fig, ax = plt.subplots()
            ax.scatter(proj[:, 0], proj[:, 1], c=labels, alpha=0.6)
            ax.set_title('KMeans (k=3) - projeção 2D (PCA)')
            kf = 'outputs/kmeans_proj.png'
            fig.savefig(kf, bbox_inches='tight')
            plt.close(fig)
            results['kmeans_image'] = kf
        except Exception:
            pass

    return results


# --- Streamlit app ---
st.set_page_config(page_title='CSV EDA Agent', layout='wide')
st.title('CSV EDA Agent — Pergunte ao seu CSV (com LLM + LangChain)')

st.sidebar.header('Config')
openai_key = st.sidebar.text_input(
    'OPENAI API KEY (ou deixe em branco e exporte OPENAI_API_KEY)',
    type='password',
    key="api_key_input"
)
if openai_key:
    os.environ['OPENAI_API_KEY'] = openai_key

st.sidebar.write('Memory e outputs serão gravados em `./memory` e `./outputs`')

uploaded = st.file_uploader(
    'Faça upload de um CSV ou ZIP',
    type=['csv', 'zip'],
    accept_multiple_files=False,
    key="upload_file"
)

# helper para extrair CSV de ZIP
def extract_csv_from_zip(uploaded_file) -> List[Tuple[str, bytes]]:
    csvs = []
    with zipfile.ZipFile(uploaded_file) as z:
        for name in z.namelist():
            if name.lower().endswith('.csv'):
                csvs.append((name, z.read(name)))
    return csvs

# fallback: CSV local
if uploaded is None:
    st.info('Você também pode usar um CSV já presente no ambiente. Ex: /mnt/data')
    if st.button('Listar arquivos em /mnt/data', key="list_files"):
        try:
            files = [f for f in os.listdir('/mnt/data')]
            st.write(files)
        except Exception as e:
            st.error(f'Erro ao listar /mnt/data: {e}')
    selected_example = st.text_input(
        'Ou cole o caminho para um CSV local (ex: /mnt/data/creditcard.csv)',
        key="local_file_input"
    )
    if selected_example:
        if os.path.exists(selected_example):
            with open(selected_example, 'rb') as f:
                uploaded = io.BytesIO(f.read())
                uploaded.name = os.path.basename(selected_example)
        else:
            st.error('Caminho inválido')

# processamento do CSV
if uploaded is not None:
    try:
        is_zip = False
        if hasattr(uploaded, 'name') and str(uploaded.name).lower().endswith('.zip'):
            is_zip = True

        if is_zip:
            uploaded.seek(0)
            ztemp = io.BytesIO(uploaded.read())
            csvs = extract_csv_from_zip(ztemp)
            if len(csvs) == 0:
                st.error('Nenhum CSV encontrado no ZIP')
            else:
                names = [n for n, _ in csvs]
                sel = st.selectbox('Escolha o CSV dentro do ZIP', names, key="csv_inside_zip")
                databytes = dict(csvs)[sel]
                df = pd.read_csv(io.BytesIO(databytes))
        else:
            uploaded.seek(0)
            df = pd.read_csv(uploaded)
    except Exception as e:
        st.error(f'Erro ao ler o arquivo: {e}')
        st.stop()

    st.success('CSV carregado com sucesso!')
    st.write(f'Nome do arquivo: {getattr(uploaded, "name", "uploaded_file")}')

    dataset_id = os.path.splitext(getattr(uploaded, 'name', 'dataset'))[0]

    st.subheader('Amostra dos dados')
    st.dataframe(df.head(200))

    if st.button('Executar EDA automática', key="run_eda"):
        st.info('Rodando análises — aguarde')
        eda = automatic_eda(df, name=dataset_id)
        memory = load_memory(dataset_id)
        memory['conclusions'].append({
            'timestamp': pd.Timestamp.now().isoformat(),
            'summary': {
                'n_rows': eda['n_rows'], 'n_cols': eda['n_cols'], 'n_outliers': eda.get('n_outliers', 0)
            }
        })
        save_memory(dataset_id, memory)
        st.write('**Resumo:**')
        st.json({k: eda[k] for k in ['n_rows', 'n_cols', 'numeric_cols', 'categorical_cols', 'n_outliers'] if k in eda})
        if 'histograms' in eda:
            st.subheader('Histogramas (amostras)')
            for p in eda['histograms']:
                st.image(p, caption=os.path.basename(p))
        if 'correlation_image' in eda:
            st.subheader('Matriz de Correlação')
            st.image(eda['correlation_image'])
        if 'kmeans_image' in eda:
            st.subheader('Clusters (projeção PCA)')
            st.image(eda['kmeans_image'])

    st.subheader('Chat / Perguntas (LLM)')
    user_question = st.text_input(
        'Pergunta para o agente LLM',
        key="user_question_input"
    )
    if st.button('Enviar pergunta', key="ask_question") and user_question:
        if 'OPENAI_API_KEY' not in os.environ or not os.environ['OPENAI_API_KEY']:
            st.error('Defina OPENAI_API_KEY no ambiente ou cole no campo à esquerda')
        else:
            try:
                llm = ChatOpenAI(temperature=0, model='gpt-3.5-turbo')
            except Exception:
                llm = None

            if LANGCHAIN_CSV_AVAILABLE and llm is not None:
                try:
                    tmp_dir = tempfile.mkdtemp()
                    csv_path = os.path.join(tmp_dir, f"{dataset_id}.csv")
                    df.to_csv(csv_path, index=False)
                    agent = create_csv_agent(
                        llm,
                        csv_path,
                        verbose=False,
                        agent_executor_kwargs={"handle_parsing_errors": True},
                        agent_type="openai-functions",
                        system_message="Você é um agente especialista em analisar arquivos csv."
                    )
                    resp = agent.run(user_question)
                    memory = load_memory(dataset_id)
                    memory['qa'].append({'question': user_question, 'answer': resp, 'timestamp': pd.Timestamp.now().isoformat()})
                    save_memory(dataset_id, memory)
                    st.subheader('Resposta do Agente (LLM)')
                    st.write(resp)
                except Exception as e:
                    st.error(f'Erro criando agent CSV: {e}')
            else:
                st.warning('LangChain CSV agent não disponível. Respondendo com análise local.')
                st.write(local_query_answer(df, user_question))

if __name__ == '__main__':
    st.write('Inicie esta aplicação com `streamlit run app.py`')
